# Email-Classification-and-Extraction-via-AI-Agent

A scalable and modular pipeline for **real-time email ingestion**, **classification**, and **structured data extraction** from various types of e-commerce emails â€” including order confirmations, shipping updates, returns, refunds, and more.

---

## ğŸš€ Built With

* âœ… Python 3.11
* âœ… LangGraph + OpenAI (GPT-4o-mini)
* âœ… Supabase (PostgreSQL + Realtime)
* âœ… Docker-ready for containerized deployment
* âœ… Modular LangGraph workflow (nodes + conditional routing)

---

## ğŸ“‚ Project Structure

```bash
email_pipeline/
â”œâ”€â”€ main.py                      # Realtime listener and orchestrator
â”œâ”€â”€ workflow/graph.py           # LangGraph flow: classify -> route -> extract
â”œâ”€â”€ nodes/                      # Category-specific extractor nodes
â”‚   â”œâ”€â”€ classify.py
â”‚   â”œâ”€â”€ order.py
â”‚   â”œâ”€â”€ shipping.py
â”‚   â”œâ”€â”€ shipping_update.py
â”‚   â”œâ”€â”€ refund.py
â”‚   â”œâ”€â”€ return_confirmation.py
â”‚   â”œâ”€â”€ return_update.py
â”œâ”€â”€ parser/email_parser.py      # HTML cleaner and item parser utils
â”œâ”€â”€ gpt/extractor.py            # GPT calling + semantic fallback match
â”œâ”€â”€ prompts/templates.py        # Prompt templates for all categories
â”œâ”€â”€ supabase_client/__init__.py # Supabase client with service role
â”œâ”€â”€ shared/types.py             # Shared LangGraph AgentState
â”œâ”€â”€ .env                        # API secrets (not committed)
â”œâ”€â”€ requirements.txt            # Python dependency list
â”œâ”€â”€ Dockerfile                  # Dockerized runtime
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ Features

* ğŸ” GPT-powered Email Classification
* âœ¨ Structured Extraction to JSON Schema
* ğŸ§  GPT fallback matching for fuzzy item linking
* ğŸ“¦ Supabase DB insert/update logic
* ğŸ”„ Real-time Realtime API event processing
* ğŸ”¹ Docker-ready for deployment

---

## âš™ï¸ Setup

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/email_pipeline.git
cd email_pipeline
```

### 2. Create `.env` File

Create a `.env` file in the root directory with:

```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-supabase-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
OPENAI_API_KEY=your-openai-api-key
```

> ğŸ“… `SUPABASE_KEY`: Used for realtime subscriptions
> ğŸ’¼ `SUPABASE_SERVICE_ROLE_KEY`: Used for DB writes securely
> ğŸ”® `OPENAI_API_KEY`: Calls GPT-4o-mini for classification + extraction

### 3. Run Locally (optional)

```bash
pip install -r requirements.txt
python main.py
```

---

## ğŸ³ Docker Instructions

### 1. Build the Image

```bash
docker build -t email_pipeline_app .
```

### 2. Run the Container

```bash
docker run -it --env-file .env email_pipeline_app
```

> ğŸ”— `-it` allows interactive logs. Use `--rm` to auto-clean after stop.

---

## ğŸ“Š Real-Time Flow Diagram

<details> <summary>Click to expand Mermaid flowchart</summary>

```mermaid
graph TD
  A[Email Inserted into Supabase] -->|Realtime Trigger| B[main.py]
  B --> C[classify node (LangGraph)]
  C --> D{category?}
  D -->|retailer order confirmation| E[order node - insert into DB]
  D -->|refund| F[refund node - match & upsert]
  D -->|retailer shipping confirmation| G[shipping node - match & update]
  D -->|return update| H[return update node]
  D -->|return confirmation| I[return confirmation node]
  D -->|shipping update| J[shipping update node]
  D -->|promos, goods receipt, services receipt| K[END - skip]
```
</details>
---

## ğŸ§  Email Categories Detected

Classifier routes each email into one of these types:

* ğŸŒŸ `promos`
* ğŸ“„ `goods receipt`
* ğŸ“¦ `retailer order confirmation`
* ğŸšš `retailer shipping confirmation`
* ğŸ’¡ `services receipt`
* ğŸ“¢ `shipping update`
* âœ… `retailer order update`
* â†©ï¸ `return confirmation`
* â™»ï¸ `return update`
* ğŸ’² `refund`

Each has a **prompt template**, **extraction schema**, and **database logic**.

---

## ğŸ¤– DB Table Summary

| Table             | Description                            |
| ----------------- | -------------------------------------- |
| `email_extracts`  | Supabase table where new emails arrive |
| `order_details`   | Parsed item-level order/shipping info  |
| `returns_refunds` | All refund and return related rows     |

---

## ğŸ” Notes

### âœï¸ 1. GPT-4o-mini Usage

* All classification and extraction is done with `gpt-4o-mini`, using structured prompts with JSON schema expectations.
* Prompts are defined in `prompts/templates.py` and customized for each category.

### ğŸ§  2. LangGraph Workflow

* We use `StateGraph` from LangGraph to define the pipeline:

  * `classify` node predicts the category.
  * Then routes to appropriate extractor node.
  * All paths end in `END` node after DB operations.
* Graph logic lives in `workflow/graph.py`

### ğŸ› ï¸ 3. Semantic Fallback Matching

* In shipping/refund flows, if DB doesn't match the item:

  * We fallback to GPT with a list of known DB entries.
  * Prompt uses fuzzy logic to find best semantic match.
  * Implemented in `gpt/extractor.py`

### ğŸ“Š 4. Conditional Inserts/Updates

* `order_details` inserts on new matches.
* `returns_refunds` does fuzzy lookups before updating rows.
* If `return_id` or `order_id` already exists in DB, we **don't overwrite** them unless DB field is null.

### ğŸ”„ 5. Supabase Realtime

* `main.py` uses Supabase's websocket listener for `INSERT` on `email_extracts`.
* As soon as a new row is inserted, the pipeline is triggered.
* Supabase credentials and keys are loaded via `.env` file.

### ğŸŒ 6. Docker Logging

* Logs from inside Docker will show:

  * `ğŸš€ Listening for database changes...`
  * `ğŸ“© New email received`
  * `âœ… Processed in X seconds`
* Use `-it` when running container to view logs in real time.

---

## âœ¨ Future Improvements

* [ ] Retry logic on OpenAI timeouts or failures
* [ ] Split out LangGraph nodes into isolated services
* [ ] Add async job queue (e.g. Celery or FastAPI BackgroundTasks)
* [ ] Add Sentry for production error tracking
* [ ] Auto-deploy with GitHub Actions + DockerHub

---

## ğŸ›¡ï¸ Security Practices

* âŒ Never commit `.env` to version control
* ğŸ” Use `SUPABASE_SERVICE_ROLE_KEY` only inside server environments
* â„ï¸ GPT key is stored securely via Docker env injection

Sure! Here's a clean and professional "About Me" section you can add at the end of your `README.md` to give credit to yourself:

---

## ğŸ‘¤ Author

**Muhammad Amaz Majid**
ğŸ§  AI Engineer | ğŸ’» Backend Developer | ğŸŒ Freelance Innovator

* ğŸ”­ Currently building intelligent automation pipelines with AI and real-time infrastructure
* ğŸŒ Based in Pakistan
* ğŸ’¬ Ask me about AI Agents, Supabase, or scalable backend architectures
* ğŸ“« Connect with me on [LinkedIn](https://www.linkedin.com/in/muhammad-amaz-majid-6715272ab) 
* ğŸ“§ Email: [amazmajid462@gmail.com](mailto:amazmajid462@gmail.com]) 

---

